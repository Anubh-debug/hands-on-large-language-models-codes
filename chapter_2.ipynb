{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anubh-debug/hands-on-large-language-models-codes/blob/main/chapter_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oiMnQvUyaN3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ITcEiAIz5Ak",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generate the text\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=20,\n",
        "  use_cache=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHXiQ9bm1Jeo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(generation_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2BefiFT0eK6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Print the output\n",
        "print(tokenizer.decode(generation_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InKiVhaF0p8F",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Let's see input_ids\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE8q4Obc04RU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Translate ids back into text\n",
        "for id in input_ids[0]:\n",
        "   print(tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv729gR21FEC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# decoding each token\n",
        "print(tokenizer.decode(3323))\n",
        "print(tokenizer.decode(622))\n",
        "print(tokenizer.decode([3323, 622]))\n",
        "print(tokenizer.decode(29901))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at how we can generate contextualized word embeddings"
      ],
      "metadata": {
        "id": "sCGJt2DkFDaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "# Load a language model\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer('Hello world', return_tensors='pt')\n",
        "\n",
        "# Process the tokens\n",
        "output = model(**tokens)[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B4LX-gJqEWUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MClM6DGfFNC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Qj34FtZ_FSwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can use models from sentence_transformers to get text embeddings of a text.\n",
        "\n"
      ],
      "metadata": {
        "id": "7hI17sooWibl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Convert text to text embeddings\n",
        "vector = model.encode(\"Best movie ever!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gf4l0i1PWbJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector.shape"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vImUUyKCWf8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using pretrained word embeddings"
      ],
      "metadata": {
        "id": "W-0cJEHwXBKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rE4fw_QXZ_Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n",
        "# Other options include \"word2vec-google-news-300\"\n",
        "# More options at https://github.com/RaRe-Technologies/gensim-data\n",
        "model = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sJttT1YZXAxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar([model['king']], topn=11)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9O_t8H4KXGX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Song recommendation using Embeddings"
      ],
      "metadata": {
        "id": "N2gcMRtVahZu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa4f0e30"
      },
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# Get the playlist dataset file\n",
        "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
        "\n",
        "# Parse the playlist dataset file. Skip the first two lines as\n",
        "# they only contain metadata\n",
        "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
        "\n",
        "# Remove playlists with only one song\n",
        "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
        "\n",
        "# Load song metadata\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
        "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
        "songs_df = songs_df.set_index('id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( 'Playlist #1:\\n ', playlists[0], '\\n')\n",
        "print( 'Playlist #2:\\n ', playlists[1])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DIceBio0aqAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train our Word2Vec model\n",
        "model = Word2Vec(\n",
        "    playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "8JNagfWBat4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song_id = 2172\n",
        "\n",
        "# Ask the model for songs similar to song #2172\n",
        "model.wv.most_similar(positive=str(song_id))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TUQjFVlOaxoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(songs_df.iloc[2172])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AT5gfrvqa32Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_recommendations(song_id):\n",
        "    similar_songs = np.array(\n",
        "        model.wv.most_similar(positive=str(song_id),topn=5)\n",
        "    )[:,0]\n",
        "    return  songs_df.iloc[similar_songs]\n",
        "\n",
        "# Extract recommendations\n",
        "print_recommendations(2172)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oZBkIzpma7OK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM9jOU2i0S0DUGiqlh9oBxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}