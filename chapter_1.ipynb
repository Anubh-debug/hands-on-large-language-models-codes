{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvtDhwuvEYG2jl1YCyo7Fb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anubh-debug/hands-on-large-language-models-codes/blob/main/chapter_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Connect to T4 in colab for faster generation"
      ],
      "metadata": {
        "id": "hH52I9KV0OIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating your first text"
      ],
      "metadata": {
        "id": "ZGtQrehVuCtT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTmSepD1sj1u"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# loading model\n",
        "model=AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True, #It tells the library to automatically select the best data type based on the model's configuration and your hardware.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading tokenizer for model\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "iiVI5oeDueKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator=pipeline(\n",
        "    \"text-generation\", #your task\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False, # by setting this as a false, it will not return prompt but only the output.\n",
        "    max_new_tokens=500, #we want the model to return only 500 tokens.\n",
        "    do_sample=False, #By setting this to False, model will not do sampling strategy to choose the next token. It will choose the next most probable token.\n",
        "    use_cache=False # Disable caching\n",
        ")"
      ],
      "metadata": {
        "id": "SCcn9xFYuuMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Default cache\n",
        "\n",
        "The DynamicCache is the default cache class for most models. It allows the cache size to grow dynamically in order to store an increasing number of keys and values as generation progresses.\n",
        "\n",
        "Disable the cache by configuring use_cache=False in generate()."
      ],
      "metadata": {
        "id": "9CuhB3BnzOX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# Generate output\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "CeZRRTOEx3-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}